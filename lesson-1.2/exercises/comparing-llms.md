# Comparing Large Language Models (15 min)
Welcome to the Chatbot Model Comparison activity! Here, you will compare the output of three different chatbot models. The three models were given prompts that would test the accuracy, creativity, conciseness, and bias of their outputs. Your job is to select the model that performed best in each category. Let's get started!

Complete the activity [here](https://igfnaqfcyl-13589482-i.codehs.me/index.html).  Then edit this page and write down your reflections here:

### Which model did you find performed best overall, and why?
I believe it's a mixture between GPT-4 and Llama. This is because, as GPT is efficient in the way it explains concepts, more difficult or insightful topics, such as being a nurse or the pro and con about emissions needs more detail to properly explain.

### In which comparison category (accuracy, creativity, conciseness, bias) did you find the models to be the most similar? What about the most different?
I'd say bias goes to Llama, as it based its data and content off of ones from the United States instead of the entire world, thus having sample bias. 

### Were you surprised by any of the results?
I was surprised of the existence of an LLM named Llama.

### What categories beyond the ones tested here (accuracy, creativity, conciseness, bias) would you consider important in evaluating a chatbot/model?
I'd say efficiency is another one to consider; a combination of all four of the categories that are being tested. A truly efficient model should be able to easily score in all of these concepts.
